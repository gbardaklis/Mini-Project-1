{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kn60bpNT6Tkt"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "uBjvWkFjC5kl"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "aJAgKDCuC-eZ"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/goemotions.json\") as f:\n",
        "    # data2 = json.loads(f.read())\n",
        "    data = json.load(f)\n",
        "\n",
        "# importing the json file into a dataframe \n",
        "df = pd.DataFrame(data, columns = ['Post', 'Emotion', 'Sentiment'])\n",
        "postsDict = df['Post'].values.tolist()\n",
        "# postsDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "j3Sy-LqBLOPg"
      },
      "outputs": [],
      "source": [
        "emotionDict = df['Emotion'].values.tolist()\n",
        "# emotionDict\n",
        "sentimentDict = df['Sentiment'].values.tolist()\n",
        "# sentimentDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "vdWgWu3IOCqi"
      },
      "outputs": [],
      "source": [
        "corpusNews = api.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyiPd7sI-x9o"
      },
      "outputs": [],
      "source": [
        "tokenCount = 0\n",
        "tokenPost = []\n",
        "\n",
        "for x in postsDict:\n",
        "  tokens = word_tokenize(x)\n",
        "  tokenPost.append(tokens)\n",
        "  tokenCount = tokenCount + len(tokens)\n",
        "print(f'The total number of tokens is {tokenCount}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7Xcn0tU1YrA"
      },
      "outputs": [],
      "source": [
        "failed = 0\n",
        "averageEmb = []\n",
        "averageEmbAll = []\n",
        "\n",
        "# Take the X TokenPost\n",
        "for index, iterator in enumerate(tokenPost):\n",
        "  for x in iterator:\n",
        "    try:\n",
        "      # Assign Corpus[x] to single word\n",
        "      singleWord = corpusNews[x]\n",
        "      # New List append that word\n",
        "      averageEmb.append(singleWord)\n",
        "    except KeyError:\n",
        "      # Doesn't fnd throw error increment counter\n",
        "      failed += 1\n",
        "  if len(averageEmb) != 0:\n",
        "    average = np.average(averageEmb, axis = 0)\n",
        "    averageEmbAll.append(average)\n",
        "    averageEmb.clear()\n",
        "  else: \n",
        "    averageEmbAll.append([0] * 300)\n",
        "# 3.3\n",
        "print(f'The failed count is: {failed}')\n",
        "\n",
        "# 3.4 \n",
        "hitRate = ((tokenCount - failed) / tokenCount) *100\n",
        "print(f'The hit rate is as follows: {hitRate}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Wg3DCbfWI0Ff"
      },
      "outputs": [],
      "source": [
        "# Open Text Word doc for Writing for BASE\n",
        "performanceTxt = open(\"performanceTxt.text\", \"w\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "6CxkCj5fP3tR"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "Xh6eXEBogfUQ"
      },
      "outputs": [],
      "source": [
        "# splitting data 80% train 20% test\n",
        "post_train, post_test, emotion_train, emotion_test, sentiment_train, sentiment_test = train_test_split = train_test_split(averageEmbAll, emotionDict, sentimentDict, test_size = 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VL6WrnaZtzcu"
      },
      "outputs": [],
      "source": [
        "# 3.5 Base-MLP with the default parameters\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "mlp = MLPClassifier(early_stopping=True)\n",
        "%time mlp.fit(post_train, emotion_train)\n",
        "%time mlp.fit(post_train, sentiment_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "ankZlfwZoKbA"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OL3XLsr1J60K"
      },
      "outputs": [],
      "source": [
        "# Write to Text Word Doc the Accuracy and the Classification Report for BASE \n",
        "performanceTxt.write(f\" The following is the information for the Base MLP Classifier \\n  \" )\n",
        "performanceTxt.write(f\" This is the Accuracy of the Tests for the emotion model \\n {mlp.score(post_test, emotion_test) * 100} \\n  \" )\n",
        "performanceTxt.write(f\" This is the Accuracy of the Tests for the sentiment model \\n {mlp.score(post_test, sentiment_test) * 100} \\n  \" )\n",
        "emotion_pred = mlp.fit(post_train, emotion_train).predict(post_test)\n",
        "performanceTxt.write(f\" This is the Classification Report for the emotion model \\n {metrics.classification_report(emotion_test, mlp.predict(post_test),labels=np.unique(emotion_test), zero_division=0)} \\n  \" )\n",
        "sentiment_pred = mlp.fit(post_train, sentiment_train).predict(post_test)\n",
        "performanceTxt.write(f\" This is the Classification Report for the sentiment model \\n {metrics.classification_report(sentiment_test, mlp.predict(post_test),labels=np.unique(sentiment_test), zero_division=0)} \\n  \" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Icq6SzzAUjUV"
      },
      "outputs": [],
      "source": [
        "# 3.6 Top-MLP using GridSearchCV\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "warnings.simplefilter('ignore', ConvergenceWarning)\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "param = {'activation' : ['logistic', 'tanh', 'relu', 'identity'], 'hidden_layer_sizes' : [(10,30,10), (50, 30)], 'solver' : ['adam', 'sgd']}\n",
        "mlp_gscv = GridSearchCV(MLPClassifier(early_stopping=True, max_iter=15, verbose=True), param)\n",
        "%time mlp_gscv.fit(post_train, emotion_train)\n",
        "%time mlp_gscv.fit(post_train, sentiment_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3HlX_-1Ja3p"
      },
      "outputs": [],
      "source": [
        "# Write to Text Word Doc the Accuracy and the Classification Report for TOP \n",
        "emotion_pred = mlp_gscv.fit(post_train, emotion_train).predict(post_test)\n",
        "sentiment_pred = mlp_gscv.fit(post_train, sentiment_train).predict(post_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Open Text Word doc for Writing for TOP\n",
        "performanceTop = open(\"performanceTop.txt\", \"w\")"
      ],
      "metadata": {
        "id": "zoeyZujCJt3P"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write to Text Word Doc the Accuracy and the Classification Report for TOP \n",
        "performanceTop.write(f\" The following is the information for the Top MLP Classifier \\n  \" )\n",
        "performanceTop.write(f\" This is the Accuracy of the Tests for the emotion model \\n {mlp.score(post_test, emotion_test) * 100} \\n  \" )\n",
        "performanceTop.write(f\" This is the Accuracy of the Tests for the sentiment model \\n {mlp.score(post_test, sentiment_test) * 100} \\n  \" )\n",
        "performanceTop.write(f\" This is the Classification Report for the emotion model \\n {metrics.classification_report(emotion_test, emotion_pred, labels=np.unique(emotion_test), zero_division=0)} \\n  \" )\n",
        "performanceTop.write(f\" This is the Classification Report for the sentiment model \\n {metrics.classification_report(sentiment_test, sentiment_pred, labels=np.unique(sentiment_test), zero_division=0)} \\n  \" )"
      ],
      "metadata": {
        "id": "BtaNdrpqJfN-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}